from copy import deepcopy
from hashlib import new
from heapq import merge
import json
from multiprocessing.sharedctypes import Value
import os
from sre_constants import REPEAT_ONE
from tempfile import tempdir
from tqdm import tqdm
import sys
from collections import Counter
import numpy as np
import re
import jieba
from datetime import datetime,date
import numpy as np


PATH = os.path.abspath(os.path.dirname(__file__))


def data_save(data, path):
    json.dump(data, open(path , 'w', encoding='utf-8'), indent=4, ensure_ascii=False)


def data_read(path):
    with open(path, 'r', encoding='utf-8') as load_f:
        load_dict = json.load(load_f)
    return load_dict


def train_data(Netease_dcit):
    datas = {}

    with tqdm(Netease_dcit.items(), desc=' Filter', file=sys.stdout, disable=False) as Netease_dcit_iterator:
        for key, value in Netease_dcit_iterator:
            # selction the comment
            comments_text = set()
            comments = value['comment']['newList']['comments']
            for i in comments:
                comment_text = comments[i]['content']
                comments_text.add(comment_text)
            comments_text = list(comments_text)

            data = {
                # 'title': value['title'],
                # 'label': value['label'],
                # 'content': value['content'],
                'comments': comments_text,
            }

            datas.update({key: data})

    data_save(datas, os.path.join(PATH, 'Netease-comment.json'))


def article_len_statistics(Netease_dcit):
    with tqdm(Netease_dcit.items(), desc=' Filter', file=sys.stdout, disable=False) as Netease_dcit_iterator:
        article_len = []
        sentence_lens = []

        for key, value in Netease_dcit_iterator:
            if value['content'] != None:
                article = value['content']
                sentence_len = 0
                for sentence in article:
                    sentence_len += len(sentence)
                sentence_lens.append(sentence_len)

        print('mean_len', np.mean(sentence_lens))
        print('median_len', np.median(sentence_lens))
        print(Counter(sentence_lens))


class ProcessingComment():
    def __init__(self) -> None:
        super(ProcessingComment, self).__init__()
        # comment_path = os.path.join(PATH, 'Netease-comment.json')
        # self.comment_dcit = data_read(comment_path)
        self.RepeatNum = 0

    def filtering_rule(self, sentence):
        # é€—å·
        sentence = re.sub(r'ï¼Œ{2,}', r'ï¼Œ', sentence)
        sentence = re.sub(r',{2,}', r',', sentence)

        # å¥å·
        sentence = re.sub(r'ã€‚{2,}', r'ã€‚', sentence)
        sentence = re.sub(r'\.{2,}', r'.', sentence)

        # é—®å·
        sentence = re.sub(r'ï¼Ÿ{2,}', r'ï¼Ÿ', sentence)
        sentence = re.sub(r'\?{2,}', r'?', sentence)

        # æ„Ÿå¹å·
        sentence = re.sub(r'ï¼{2,}', r'ï¼', sentence)
        sentence = re.sub(r'!{2,}', r'!', sentence)

        # å†’å·
        sentence = re.sub(r'ï¼š{2,}', r'ï¼š', sentence)
        sentence = re.sub(r':{2,}', r':', sentence)

        # åˆ†å·
        sentence = re.sub(r'ï¼›{2,}', r'ï¼›', sentence)
        sentence = re.sub(r';{2,}', r';', sentence)

        # é¡¿å·
        sentence = re.sub(r'ã€{1,}', r'ã€', sentence)

        # æ‹¬å·
        sentence = re.sub(r'ï¼ˆ{2,}', r'ï¼ˆ', sentence)
        sentence = re.sub(r'ï¼‰{2,}', r'ï¼‰', sentence)
        sentence = re.sub(r'\({2,}', r'(', sentence)
        sentence = re.sub(r'\){2,}', r')', sentence)

        sentence = re.sub(r'ã€{2,}', r'ã€', sentence)
        sentence = re.sub(r'ã€‘{2,}', r'ã€‘', sentence)
        sentence = re.sub(r'\[{2,}', r'[', sentence)
        sentence = re.sub(r']{2,}', r']', sentence)

        sentence = re.sub(r'\{{2,}', r'{', sentence)
        sentence = re.sub(r'}{2,}', r'}', sentence)

        sentence = re.sub(r'\\{2,}', r'\\', sentence)

        sentence = re.sub(r'\|{2,}', r'|', sentence)

        sentence = re.sub(r'/{2,}', r'/', sentence)

        # ç ´æŠ˜å·
        sentence = re.sub(r'\-{1,}', r'-', sentence)
        sentence = re.sub(r'â€”{1,}', r'-', sentence)

        # çœç•¥å·
        sentence = re.sub(r'â€¦{1,}', r'-', sentence)

        # ä¹¦åå·
        sentence = re.sub(r'ã€Š{2,}', r'ã€Š', sentence)
        sentence = re.sub(r'ã€‹{2,}', r'ã€‹', sentence)
        sentence = re.sub(r'<{2,}', r'<', sentence)
        sentence = re.sub(r'>{2,}', r'>', sentence)

        # ç‰¹æ®Šç¬¦å·
        sentence = re.sub(r'%{2,}', r'%', sentence)

        sentence = re.sub(r'@{2,}', r'@', sentence)

        sentence = re.sub(r'#{2,}', r'#', sentence)

        sentence = re.sub(r'ï¿¥{2,}', r'ï¿¥', sentence)
        sentence = re.sub(r'\${2,}', r'$', sentence)

        sentence = re.sub(r'&{2,}', r'&', sentence)


        sentence = re.sub(r'\^{1,}', r'', sentence)

        sentence = re.sub(r'\*{1,}', r'', sentence)

        sentence = re.sub(r'_{1,}', r'', sentence)

        sentence = re.sub(r'={1,}', r'', sentence)

        sentence = re.sub(r'\+{1,}', r'', sentence)

        # æ›¿æ¢ \n \r \t
        sentence = sentence.replace('\n', '')
        sentence = sentence.replace('\r', '')
        sentence = sentence.replace('\\n', '')
        sentence = sentence.replace('\\r', '')

        sentence = sentence.replace('\t', '')

        sentence = sentence.replace('ã€Œ', '')
        sentence = sentence.replace('ã€', '')

        sentence = sentence.replace('. ', '')
        sentence = sentence.replace('ã€‚ ', '')

        sentence = sentence.replace('â”', '')
        sentence = sentence.replace('â”“', '')

        sentence = sentence.replace('ğŸ«¡', '')

        sentence = sentence.replace('â—', '')
        sentence = sentence.replace('Â°', '')
        sentence = sentence.replace('â—†', '')
        sentence = sentence.replace('â– ', '')
        sentence = sentence.replace('. ', '')

        sentence = sentence.replace('. ', '')

        sentence = re.sub(r'ã€€{1,}', r'', sentence)



        # å»æ‰ç©ºæ ¼
        sentence = re.sub(r'^ {1,}', r' ', sentence)
        sentence = re.sub(r' {1,}$', r' ', sentence)

        return sentence

    def filter_repeat_sentence(self, comments):
        """
        å»é™¤listä¸­é‡å¤çš„å†…å®¹
        """
        list_len, set_len = len(comments), len(set(comments))

        # è‹¥listçš„é•¿åº¦å’Œé›†åˆé•¿åº¦ä¸ä¸€è‡´ï¼Œè¯´æ˜å­˜åœ¨é‡å¤çš„å¥å­ï¼Œåˆ©ç”¨setå»é‡ç„¶åæ ¹æ® list çš„index çš„
        if list_len != set_len:
            comments = sorted(set(comments), key=comments.index)
            self.RepeatNum += 1

        return comments

    def filter_subset_comment(self, comments):
        new_comments = deepcopy(comments)

        for short_sentence, _ in comments.items():
            for long_sentence, _  in comments.items():
                if not short_sentence == long_sentence:
                    if short_sentence in long_sentence and len(short_sentence) >= 5:
                        try:
                            # ç”±äºé¦–æ¬¡ç¬¦åˆè¯¥æ¡ä»¶çš„å¥å­å·²ç»è¢«åˆ é™¤äº†ï¼Œè€Œåç»­å¥å­è¿˜æ˜¯äººå®¶å­é›†å¥å­ï¼Œåˆ é™¤æŠ¥é”™
                            del new_comments[short_sentence]
                            # print(short_sentence)
                        except:
                            # print(short_sentence, 'not is exist!!!')
                            pass

        return new_comments

    def filter_comment_min_len(self, comment):
        """
        æŒ‰æœ€å°é•¿åº¦è¿‡æ»¤è¯„è®º
        """
        if len(comment) < 5:
            comment = None

        return comment

    def filter_comment_max_len(self,comments):
        new_comments = deepcopy(comments)

        for sentence, _  in comments.items():
            sentence_list = sentence.split(' ')
            if len(sentence_list) > 60 :
                del new_comments[sentence]

        return new_comments

    def segmentation(self, comment):
        # è¿›è¡Œåˆ†è¯æ“ä½œ
        segmentation_comment = jieba.cut(comment, cut_all=False)
        segmentation_comment = " ".join(segmentation_comment)

        return segmentation_comment

    def post_processing(self, comment):
        filter_comment = re.sub(r' {1,}', r' ', comment)

        return filter_comment

    def get_N_longest_comment(self, comments):
        """
        è·å–è¯„è®ºä¸­é•¿åº¦å‰ 10 çš„æ–‡æœ¬
        """
        new_comments = {}

        comment_contents = list(comments.keys())
        comment_contents = sorted(comment_contents, key = lambda i:len(i), reverse=True)
        comment_contents = comment_contents[:10]

        for new_key in comment_contents:
            value = comments[new_key]
            new_comments.update({new_key: value})

        return new_comments

    def filter_comment_content(self, comments_dict):
        """ å¤„ç†å•æ¡è¯„è®º """
        new_comments = {}

        for key, value in comments_dict.items():
            # å»æ‰è¯„è®ºä¸­<br>æ ‡ç­¾
            content = value['content']
            content = re.sub(r'<br>|<br>[0-9]|br', '', content)

            # å¤„ç†æ ‡ç‚¹ç¬¦å·é‡å¤é—®é¢˜
            content = self.filtering_rule(content)

            # å¯¹è¯„è®ºæ–‡æœ¬çš„æœ€å°é•¿åº¦åšé™åˆ¶
            content = self.filter_comment_min_len(content)

            # æŠ›å¼ƒè¿‡æ»¤åä¸ºç©ºçš„è¯„è®º
            if content != None:
                # å¯¹è¯„è®ºåˆ†è¯
                content = self.segmentation(content)

                # è¯„è®ºå†…å®¹åå¤„ç†
                content = self.post_processing(content)

            value['content'] = content
            new_comments.update({key: value})

        return new_comments


class ProcessingArticle(object):
    def __init__(self) -> None:
        # article_path = os.path.join(PATH, 'Netease-article.json')
        # self.article_dcit = data_read(article_path)

        self.Chars = set()
        self.RepeatNum = 0

        self.total_article_len = 0
        self.total_article_sentence_num = 0
        self.total_article_sentence_len = 0

    def statistical_char(self, new_article):
        for sentence in new_article:
            for char in sentence:
                self.Chars.add(char)

    def statistical_article_lenth(self, new_article):
            self.total_article_len += len(new_article[-1])
            pass

    def statistical_article_sentence_num(self, new_article):
            self.total_article_sentence_num += len(new_article)
            pass

    def statistical_article_sentence_len(self, new_article):
            single_article_len = 0
            for sentence in new_article:
                single_article_len += len(sentence)

            if new_article != []:
                self.total_article_sentence_len = single_article_len // len(new_article)
                pass

    # ----------------------

    def sentence_split(self, article):
        """
        ä¸­æ–‡å¥å­åˆ’åˆ†æ ‡è¯†ï¼š
        """
        # (1) ã€‚
        new_article = []
        for sentence in article:
            sentence_list = sentence.split('ã€‚')
            sentence_list = [item + 'ã€‚' for item in sentence_list]
            new_article.extend(sentence_list)

        # # (2) ï¼Ÿ
        # article = new_article
        # new_article = []
        # for sentence in article:
        #     if 'ï¼Ÿ' in sentence:
        #         sentence_list = sentence.split('ï¼Ÿ')
        #         sentence_list_add_mark = [sentence_list[i] + 'ï¼Ÿ' for i in range(len(sentence_list) - 1)]
        #         sentence_list_add_mark.append(sentence_list[-1])
        #         new_article.extend(sentence_list_add_mark)
        #     else:
        #         new_article.append(sentence)

        # # (3) ï¼
        # article = new_article
        # new_article = []
        # for sentence in article:
        #     if 'ï¼' in sentence:
        #         sentence_list = sentence.split('ï¼')
        #         sentence_list_add_mark = [sentence_list[i] + 'ï¼' for i in range(len(sentence_list) - 1)]
        #         sentence_list_add_mark.append(sentence_list[-1])
        #         new_article.extend(sentence_list_add_mark)
        #     else:
        #         new_article.append(sentence)

        # å¯¹æ–‡ç« æ¸…æ´—ï¼Œå½“é¢å¯¹æ ‡ç‚¹ç¬¦å·ç‹¬è‡ªå æ®ä¸€è¡Œæ—¶
        punctuation = ['ã€‚', 'ï¼Ÿ', 'ï¼']
        for sentence in new_article:
            if sentence in punctuation:
                try:
                    new_article.remove(sentence)
                except:
                    pass

        # å¯¹è¯„è®ºé•¿åº¦çš„ä¸‹é™åšé™åˆ¶
        for sentence in new_article:
            if len(sentence) < 5 :
                new_article.remove(sentence)

        return new_article

    def article_len_statistics(self, Netease_dcit):
        with tqdm(Netease_dcit.items(), desc=' Filter', file=sys.stdout, disable=False) as Netease_dcit_iterator:
            article_len = []
            sentence_lens = []

            for key, value in Netease_dcit_iterator:
                if value['content'] != None:
                    article = value['content']
                    sentence_len = 0
                    for sentence in article:
                        sentence_len += len(sentence)
                    sentence_lens.append(sentence_len)

            print('mean_len', np.mean(sentence_lens))
            print('median_len', np.median(sentence_lens))
            print(Counter(sentence_lens))

    def filter_keywords(self, keywords, sentence):
        flag = False
        for item in keywords:
            if item in sentence and len(sentence) < 60:
                flag = True

        return flag

    def filter_subset_sentence(self, new_article):
        """
        è¿‡æ»¤æ‰listä¸­å…¶ä»–åŒ…å«å…¶ä»–ä½ç½®å­é›†å†…å®¹
        """
        filter_subset_article = deepcopy(new_article)

        for short_sentence in new_article:
            for long_sentence in new_article:
                if not short_sentence == long_sentence:
                    if short_sentence in long_sentence and len(short_sentence) > 30:
                        try:
                            # ç”±äºé¦–æ¬¡ç¬¦åˆè¯¥æ¡ä»¶çš„å¥å­å·²ç»è¢«åˆ é™¤äº†ï¼Œè€Œåç»­å¥å­è¿˜æ˜¯è¯¥å­é›†å¥å­ï¼Œåˆ é™¤æŠ¥é”™
                            filter_subset_article.remove(short_sentence)
                        except:
                            # print(short_sentence, 'not is exist!!!')
                            pass

        return filter_subset_article

    def filtering_rule(self, sentence):
        """ ç»Ÿä¸€é‡‡ç”¨ä¸­æ–‡çš„æ ‡ç‚¹ç¬¦å· """
        # è‹±æ–‡æ ‡ç‚¹è½¬ä¸­æ–‡æ ‡ç‚¹ ï¼ ï¼Ÿï¼Œ {2,}é‡‡ç”¨ä»1åŒ¹é…

        # é€—å·
        sentence = re.sub(r'ï¼Œ{1,}', r'ï¼Œ', sentence)
        sentence = re.sub(r',{1,}', r'ï¼Œ', sentence)

        # å¥å·
        sentence = re.sub(r'ã€‚{1,}', r'ã€‚', sentence)
        sentence = re.sub(r'\.{1,}', r'.', sentence)

        # é—®å·
        sentence = re.sub(r'ï¼Ÿ{1,}', r'ï¼Ÿ', sentence)
        sentence = re.sub(r'\?{1,}', r'ï¼Ÿ', sentence)

        # æ„Ÿå¹å·
        sentence = re.sub(r'ï¼{1,}', r'ï¼', sentence)
        sentence = re.sub(r'!{1,}', r'ï¼', sentence)

        # å†’å·
        sentence = re.sub(r'ï¼š{1,}', r'ï¼š', sentence)
        sentence = re.sub(r':{1,}', r'ï¼š', sentence)

        # åˆ†å·
        sentence = re.sub(r'ï¼›{1,}', r'ï¼›', sentence)
        sentence = re.sub(r';{1,}', r'ï¼›', sentence)

        # é¡¿å·
        sentence = re.sub(r'ã€{1,}', r'ã€', sentence)

        # æ‹¬å·
        sentence = re.sub(r'ï¼ˆ{1,}', r'ï¼ˆ', sentence)
        sentence = re.sub(r'ï¼‰{1,}', r'ï¼‰', sentence)
        sentence = re.sub(r'\({1,}', r'ï¼ˆ', sentence)
        sentence = re.sub(r'\){1,}', r'ï¼‰', sentence)

        sentence = re.sub(r'ã€{1,}', r'\[', sentence)
        sentence = re.sub(r'ã€‘{1,}', r']', sentence)
        sentence = re.sub(r'\[{1,}', r'\[', sentence)
        sentence = re.sub(r']{1,}', r']', sentence)

        sentence = re.sub(r'\{{1,}', r'{', sentence)
        sentence = re.sub(r'}{1,}', r'}', sentence)

        sentence = re.sub(r'\\{1,}', r'\\', sentence)

        sentence = re.sub(r'\|{1,}', r'|', sentence)

        sentence = re.sub(r'/{1,}', r'/', sentence)

        # ç ´æŠ˜å·
        sentence = re.sub(r'\-{1,}', r'-', sentence)
        sentence = re.sub(r'â€”{1,}', r'-', sentence)


        # çœç•¥å·
        sentence = re.sub(r'â€¦{1,}', r'-', sentence)

        # ä¹¦åå·
        sentence = re.sub(r'ã€Š{1,}', r'ã€Š', sentence)
        sentence = re.sub(r'ã€‹{1,}', r'ã€‹', sentence)
        sentence = re.sub(r'<{1,}', r'<', sentence)
        sentence = re.sub(r'>{1,}', r'>', sentence)

        # ç‰¹æ®Šç¬¦å·
        sentence = re.sub(r'%{1,}', r'%', sentence)

        sentence = re.sub(r'@{1,}', r'@', sentence)

        sentence = re.sub(r'#{1,}', r'#', sentence)

        sentence = re.sub(r'ï¿¥{1,}', r'ï¿¥', sentence)
        sentence = re.sub(r'\${1,}', r'$', sentence)

        sentence = re.sub(r'&{1,}', r'&', sentence)


        sentence = re.sub(r'\^{1,}', r'', sentence)

        sentence = re.sub(r'\*{1,}', r'', sentence)

        sentence = re.sub(r'_{1,}', r'', sentence)

        sentence = re.sub(r'={1,}', r'', sentence)

        sentence = re.sub(r'\+{1,}', r'', sentence)

        # æ›¿æ¢ \n \r \t
        sentence = sentence.replace('\n', '')
        sentence = sentence.replace('\r', '')
        sentence = sentence.replace('\\n', '')
        sentence = sentence.replace('\\r', '')

        sentence = sentence.replace('\t', '')

        sentence = sentence.replace('ã€Œ', '')
        sentence = sentence.replace('ã€', '')

        # å»æ‰ç©ºæ ¼
        sentence = re.sub(r' {1,}', r' ', sentence)

        # ç§»é™¤ä¸å¯è§çš„å­—ç¬¦
        new_sentence = []
        for item in sentence:
            is_printable_char = item.isprintable()
            if is_printable_char:
                new_sentence.append(item)

        sentence = ''.join(new_sentence)

        return sentence

    def filter_article_content(self, article):
        new_article = []

        for sentence in article:
            sentence = self.filtering_rule(sentence)

            if sentence != '':
                new_article.append(sentence)

        return new_article

    def filter_repeat_sentence(self, article):
        """
        å»é™¤listä¸­é‡å¤çš„å†…å®¹
        """
        list_len, set_len = len(article), len(set(article))

        # è‹¥listçš„é•¿åº¦å’Œé›†åˆé•¿åº¦ä¸ä¸€è‡´ï¼Œè¯´æ˜å­˜åœ¨é‡å¤çš„å¥å­ï¼Œåˆ©ç”¨setå»é‡ç„¶åæ ¹æ® list çš„index çš„
        if list_len != set_len:
            article = sorted(set(article), key=article.index)
            self.RepeatNum += 1

        return article

    def add_extra_punctuation(self, article):
        """
        listä¸­æ¯ä¸ªå¥å­æ˜¯å¦éƒ½å¸¦æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œæ–¹ä¾¿åç»­å¥å­åˆ‡åˆ†

        """
        punctuation = ['ã€‚', '.',
                    'ï¼Œ', ',',
                    'ï¼', '!',
                    'ã€',
                    'ï¼›', ';',
                    'ï¼š', ':',
                    'ï¼Ÿ', '?',
                    ]

        new_article = []

        # é’ˆå¯¹filter_subset_article ä¸­listä¸€è¡Œæ²¡æœ‰æ ‡ç‚¹åˆ†å‰²ç¬¦å·ï¼Œé¢å¤–æ·»åŠ 
        for index, sentence in enumerate(article):

            # è·å¾—å…ƒç´ ä¸­æ¯ä¸ªå¥å­ä¸­æœ€åçš„å­—ç¬¦
            end_char = sentence[-1]

            if end_char in punctuation:
                # è‹¥å­˜åœ¨ç»™å®šå­—ç¬¦é›†ä¸­ï¼Œä¸åšå¤„ç†
                # print(end_char)
                pass
            else:
                # è‹¥æ˜¯listä¸­æœ€åä¸€ä¸ªå…ƒç´ ï¼Œåˆ™èµ‹å€¼ä¸ºå¥å·ï¼Œå¦åˆ™èµ‹å€¼ä¸ºé€—å·
                if index != len(article) - 1:
                    sentence += 'ï¼Œ'
                else:
                    sentence += 'ã€‚'
            new_article.append(sentence)

        return new_article

    def segmentation(self, article):
        # è¿›è¡Œåˆ†è¯æ“ä½œ
        segmentation_article = []

        for sentence in article:
            sentence = jieba.cut(sentence, cut_all=False)
            sentence = " ".join(sentence)
            segmentation_article.append(sentence)

        return segmentation_article

    def post_processing(self, article):
        post_keywords = [
            'ï¼ˆä¸­æ–°è´¢ç»ï¼‰', 'æœç´¢ï¼Œå¤åˆ¶', 'ï¼ˆå¸ƒå¸ƒï¼‰', 'ï¼ˆå¥‹æ–—ä¹Œæ‰˜é‚¦/æ–‡ï¼‰', 'ï¼ˆæµ·å¤–ç½‘', 'ï¼ˆæ€»å°è®°è€…', 'ï¼ˆå¤®è§†è®°è€…', 'ï¼ˆä¸“æ ä½œå®¶ï¼‰',\
            'ï¼ˆå‚ä¸è®°è€…', 'ï¼ˆç¼–è¯‘', 'ï¼ˆå£°æ˜ï¼š', 'ï¼ˆå¤‡æ³¨ï¼š', 'ï¼ˆæ–‡çŒ›ï¼‰', 'ï¼ˆå”é£ï¼‰', 'ï¼ˆä½œè€…', 'ï¼ˆæ€»å°å¤®è§†è®°è€…', 'ï¼ˆä½Ÿæ’ï¼‰', 'ï¼ˆä¼Šä¸‡ï¼‰',\
            'ï¼ˆå¡å°”å‰å¥¥ï¼‰', 'ï¼ˆæ–¯ç§‘ï¼‰', 'ï¼ˆèŒ‰è‰ï¼‰', 'ï¼ˆæ–‡ç„ï¼‰', 'ï¼ˆå¡å¡ï¼‰', 'ï¼ˆæ£‹ç‰Œæ·±åº¦æŠ¥é“ç»„', 'ï¼ˆèƒ¡æ³¢ï¼‰', 'ï¼ˆå°é£ï¼‰', 'æ’ç‰ˆï¼š',\
            'ï¼ˆæ¸…æ³ ï¼‰', 'ï¼ˆä½³ä½³ï¼‰', 'ï¼ˆäºŒé¥¼ï¼‰', 'æ‘„å½±ï¼š', 'ï¼ˆæ¾æ¹ƒæ–°é—»è®°è€…', 'ï¼ˆç¦å¸ƒæ–¯ï¼‰', 'è§†è§‰ä¸­å›½ï¼‰', 'ï¼ˆå­å¥ï¼‰', 'ï¼ˆæ˜Ÿæµ·ï¼‰',\
            'ï¼ˆæ¾æ¹ƒæ–°é—»è®°è€…', 'ï¼ˆæ³•å¾‹å­¦è€…ï¼‰', 'ï¼ˆæ–‡ä¸­å—è®¿', 'ï¼ˆæ€»å°è®°è€…', 'ï¼ˆå‚ä¸è®°è€…', 'ï¼ˆé¡¾é”¦ä¸œï¼‰', 'ï¼ˆææ™“ï¼‰', 'ï¼ˆæ±Ÿé›ªï¼‰', 'ï¼ˆæŸ³ç‰é¹ï¼‰',\
            'ï¼ˆä¸­æ–°ç»çº¬APPï¼‰', 'ç¼–æ’°/', 'ï¼ˆåº”å—è®¿è€…è¦æ±‚', 'è®¾è®¡ï¼š', 'ï¼ˆä¸­é’æŠ¥', 'ï¼ˆäººæ°‘æ—¥æŠ¥ï¼‰', 'ï¼ˆç¼–è¾‘/', 'ï¼ˆä¸­æ–°ç»çº¬APPï¼‰', 'ç›‘  åˆ¶ï¼š',\
            'ï¼ˆåˆ˜æ—­ï¼‰', 'ä½œè€…/', 'ï¼ˆå¤®è§†æ–°é—»ï¼‰',
        ]

        for sentence in article:
            if len(sentence) < 30:
                for item in post_keywords:
                    if item in sentence:
                        try:
                            article.remove(sentence)
                        except:
                            # print(sentence)
                            pass

        return article

    def filter_article_text(self, article):
        # åˆå§‹åŒ– æ–°çš„æ–‡ç« åˆ—è¡¨
        new_article = []
        filter_content = []

        keywords = ['|', 'ï½œ', 'https', 'https://', 'http', 'http://', 'ä¸¨', 'ç‰ˆæƒå£°æ˜', 'å…è´£å£°æ˜', 'æœªç»è®¸å¯', 'æœªç»æˆæƒ', 'æˆæƒè½¬è½½åˆä½œè”ç³»äºº', \
        'â–²', 'â–³', 'å›¾ æºä¸¨', 'å›¾æºï¼š','å›¾æº/', 'æ–°åª’ä½“æ’ç‰ˆ', 'END',\
        'ç¼–è¾‘ï¼š', 'ç¼– è¾‘ä¸¨', 'ç¼–è¾‘/', 'è´£ç¼–ï¼š', 'ç¼–è¾‘ ', 'è´£ç¼–', 'ä¸»ç¼–', 'ä¸»ç¼–|', 'ä½œè€…ï¼š', 'ä½œ è€…ä¸¨', 'ä½œè€…ä¸¨', 'ä½œè€…|','ç¼– è¾‘ä¸¨', 'ç¼–è¾‘ï¼', 'æœ¬æœŸç¼–è¾‘', 'è§ä¹ è®°è€… ', \
        'ï¼ˆåŸæ ‡é¢˜ï¼š', 'æ–‡ä¸¨', 'æ–‡ |', 'æ–‡/', 'æ–‡ï¼š', 'èµ„æ–™å›¾', 'å…¬ä¼—å·ï¼š', 'æ–°æµªå¾®åš /', 'å°é¢å›¾ç‰‡ï¼š', 'è½¬è½½è¯·æ³¨æ˜',
        'å‡ºå“|', 'åå•†éŸ¬ç•¥', 'éƒ¨åˆ†å›¾ç‰‡æ¥æºäºç½‘ç»œ', 'å¦‚æ¶‰åŠä¾µæƒ', 'ç‰ˆæƒæ‰€æœ‰', 'è¯·è”ç³»åˆ é™¤', 'åŸåˆ›æ–‡ç« ', 'å¦‚æœ‰ä¾µæƒ', 'å¦‚éœ€è½¬è½½', 'ä¾µæƒå¿…ç©¶', \
        'æœ¬æ–‡æ¥æºï¼š', 'æ¥æº:', 'å›¾ç‰‡æ¥æº', 'å›¾æºç½‘ç»œ', 'åŸæ–‡æ ‡é¢˜ï¼š', 'å›¾ç‰‡æºäºç½‘ç»œ', 'åº”é‡‡è®¿å¯¹è±¡è¦æ±‚', 'æ ¡å¯¹ï¼š', 'ç›‘åˆ¶ï¼š', 'ä¸­å›½åŸºé‡‘æŠ¥', 'ä½œè€…ï¼š',\
        ]

        # å»æ‰å«æœ‰æ— ç”¨å…³é”®å­—çš„å¥å­
        for sentence in article:
            # åŒ¹é… XXX è®°è€…ç±»
            match_journalist = re.match(r'^.*?è®°è€… |^.*?è®°è€…ï¼š|^.*?è´¢ç» |^.*?è´¢ç»å‡ºå“ |^.*?æ¥æºï¼š|ç½‘æ˜“ä½“è‚².*?æŠ¥é“', sentence)

            # åŒ¹é… [1] ....ï¼Œå‚è€ƒå¼•ç”¨å‹å¥å­
            matchObj = re.match(r'\[\d\]|ï¼ˆåŸæ ‡é¢˜ï¼š|^ï¼ˆç¼–è¾‘', sentence)

            # è¿‡æ»¤æ‰url
            Url_obj = re.compile(r'[a-zA-z]+://[^\s]*')
            url = Url_obj.findall(sentence)

            # è¿‡æ»¤ é‚®ç®±
            E_mail_obj = re.compile(r"[\w!#$%&'*+/=?^_`{|}~-]+(?:\.[\w!#$%&'*+/=?^_`{|}~-]+)*@(?:[\w](?:[\w-]*[\w])?\.)+[\w](?:[\w-]*[\w])?")
            e_mail = E_mail_obj.findall(sentence)

            # è¿‡æ»¤ ç”µè¯å·ç 
            Phone_num = re.compile(r"\d{3}-\d{8}|\d{4}-\{7,8}")
            phone_num = Phone_num.findall(sentence)

            # 1. å»æ‰ä¸ºç©ºçš„å¥å­
            if len(sentence) == 0:
                continue

            # 2. å»æ‰ è®°è€… XXX
            elif match_journalist:
                if len(sentence) <=60:
                    # åŒ¹é…æˆåŠŸä½†å°äº 60åˆ™è¿‡æ»¤æ‰
                    # print(sentence)
                    filter_content.append(sentence)
                    pass
                else:
                    # å°†åŒ¹é…æˆåŠŸçš„å¥å­å¤§äº60çš„ç›´æ¥æ·»åŠ åˆ° æ–°æ–‡ç« åˆ—è¡¨ä¸­
                    new_article.append(sentence)

            elif url != []:
                # print(sentence)
                continue

            elif e_mail != []:
                # print(sentence)
                continue

            elif phone_num != []:
                # print(sentence)
                continue

            else:
                # 3. äºŒæ¬¡æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤
                if matchObj:
                    # åŒ¹é…æˆåŠŸåˆ™è¿‡æ»¤æ‰è¯¥å¥å­
                    filter_content.append(sentence)
                    # print(sentence)
                    pass

                else:
                    # 4. åˆ¤æ–­è¯¥å¥å­æ˜¯å¦è¿˜å­˜åœ¨keywordså…³é”®æŒ‡å®šçš„å†…å®¹ï¼Œæœ‰åˆ™è¿‡æ»¤
                    flag = self.filter_keywords(keywords, sentence)
                    if not flag:
                        new_article.append(sentence)
                    else:
                        filter_content.append(sentence)
                        # print(sentence)

        # å»æ‰é‡å¤çš„å¥å­
        new_article = self.filter_repeat_sentence(new_article)

        # å»æ‰å«æœ‰åŒ…å«å…³ç³»çš„å¥å­ï¼Œå³å»æ‰å­é›†å¥å­
        new_article = self.filter_subset_sentence(new_article)

        # å†…å®¹è¿‡æ»¤,
        new_article = self.filter_article_content(new_article)

        # å¢åŠ é¢å¤–çš„æ ‡ç‚¹ç¬¦å·
        new_article = self.add_extra_punctuation(new_article)

        # åˆå¹¶ä¸ºä¸€æ®µæ–‡æ¡£
        new_article = [''.join(new_article)]

        # å¥å­å±‚çº§åˆ’åˆ†
        new_article = self.sentence_split(new_article)

        # åå¤„ç†å»æ‰ä¹‹å‰å¤„ç†ä¸æ‰çš„å…³é”®è¯
        new_article = self.post_processing(new_article)

        # åˆ†è¯æ“ä½œ
        new_article = self.segmentation(new_article)

        return new_article, filter_content

    def forword(self, ):
        filter_contents = []
        datas = {}

        for key, value in self.article_dcit.items():
            article = value['content']
            if article != None:
                new_article, filter_content = self.filter_article_text(article)

                data = {'content': new_article}
                datas.update({key: data})

                # å­˜ç•™ è¿‡æ»¤å¥å­å’Œå­—ç¬¦
                filter_contents.extend(filter_content)


        print('repeat sentence number', self.RepeatNum)
        print('total article number', len(datas))
        print(f'article average length { self.total_article_len / len(datas)}')
        print(f'article sentence average number { self.total_article_sentence_num / len(datas)}')
        print(f'article sentence average length { self.total_article_sentence_len / len(datas)}')
        # print('Char:', self.Chars)

        # data_save(datas, os.path.join(PATH, 'NeteaseFilterArticle.json'))
        # data_save(list(set(filter_contents)), os.path.join(PATH, 'NeteaseFilterContent.json'))


class Processing():
    def __init__(self) -> None:
        # Sina_userinfo ç¬¬ä¸€æ¬¡å®Œæ•´è·‘å®Œæ‰€æœ‰æµç¨‹å¹¶ç»˜åˆ¶å¯è§†åŒ–å›¾åƒçš„æ•°æ®é›†ï¼Œå¹¶ä¸ä½œä¸ºåæœŸè®­ç»ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ã€‚
        self.main_path = os.path.join(PATH, 'Sina_userinfo')

        self.Sina = {}
        self.age_frequency = []
        self.gender_frequency = []
        self.location_frequency = []

        self.processingarticle = ProcessingArticle()
        self.processingcomment = ProcessingComment()

    def processing_article_text(self, article):
        new_article, _ = self.processingarticle.filter_article_text(article)

        # è‹¥å†…å®¹ä¸ºç©ºåˆ™è¿”å›None
        if len(new_article) == 0:
            new_article = None

        return new_article

    def processing_title_text(self, title):
        title.replace(' ', '')

        title = jieba.cut(title, cut_all=False)
        title = " ".join(title)

        return title

    def processing_keywords(self, keywords):
        keywords = keywords.split(',')
        new_keywords = []

        for item in keywords:
            if item != '':
                new_keywords.append(item)
        return new_keywords

    def get_comment_other_info(self, comments_dict):
        """
        ç‚¹èµæ•°é‡ vote
        ç‚¹è¸©æ•°é‡ against
        ç”¨æˆ·ä¿¡æ¯ user
            ç”¨æˆ·æ˜µç§° nickname
            ç”¨æˆ·ä½ç½® location
            ç”¨æˆ·å¤´åƒ  avatar
            å¤´è¡”ç­‰çº§ titleinfo
        """
        new_comments_dict = {}

        for sentence, value in comments_dict.items():
            # å…ˆåˆ¤æ–­æ˜¯å¦å­˜åœ¨è¿™äº›å­—æ®µï¼Œæ²¡æœ‰åˆ™èµ‹å€¼åˆå§‹åŒ–çš„å€¼
            if 'vote' in value:
                vote = value['vote']
            else:
                vote = 0

            if 'against' in value:
                against = value['against']
            else:
                against = 0

            content = value['content']

            # åˆå§‹åŒ– user
            user = {}
            if 'user' in value:
                if 'nickname' in value['user']:
                    nickname = value['user']['nickname']
                else:
                    nickname = ''

                if 'location' in value['user']:
                    location = value['user']['location']
                else:
                    location = ''

                if 'avatar' in value['user']:
                    avatar = value['user']['avatar']
                else:
                    avatar = ''

                if 'titleinfo' in value['user']:
                    titleinfo = value['user']['titleinfo']
                else:
                    titleinfo = ''

                user.update({'nickname': nickname})
                user.update({'location': location})
                user.update({'avatar': avatar})
                user.update({'titleinfo': titleinfo})


            new_value = {'vote': vote,
                        'against': against,
                        'content': content,
                        'user': user
                        }

            new_comments_dict.update({sentence: new_value})

        return new_comments_dict

    def calculate_age(self, born):
        # ç»™å®šå…·ä½“çš„å‡ºç”Ÿæ—¥æœŸè®¡ç®—å¹´é¾„
        today = date.today()
        age = today.year - born.year - ((today.month, today.day) < (born.month, born.day))
        return age

    def processing_birthday(self, birthday):
        # é˜²æ­¢ æœˆ æ—¥ä¸º00-00çš„æƒ…å†µï¼Œåˆå§‹åŒ–ä¸º01-01
        years_month_day = birthday.split('-')
        years = years_month_day[0]
        month = years_month_day[1]
        day = years_month_day[-1]
        if month == '00':
            month = '01'
        if day == '00':
            day = '01'
        birthday = '-'.join([years, month, day])

        return birthday

    def processing_userinfo(self, comment):
        for key, value in comment.items():

            # 1. å¤„ç† birthdayï¼Œ ç›®æ ‡ æ•°å­—å¹´é¾„
            birthday = value['userinfo']['birthday'].split(' ')[0]

            # å°†éå‡ºç”Ÿæ—¥æœŸçš„æ’é™¤
            if len(birthday) > 3:
                birthday = self.processing_birthday(birthday)

                birth_date = datetime.strptime(birthday, '%Y-%m-%d')
                age = self.calculate_age(birth_date)

                # å°†å¹´é¾„èŒƒå›´é™åˆ¶åˆ° 1-100ä¹‹é—´ï¼ŒæŠŠç¬¦åˆè¦æ±‚çš„æ·»åŠ å¹´é¾„å­—æ®µ
                if age > 0 and age < 100:
                    self.age_frequency.append(age)
                    value['userinfo'].update({'age': age})

            # 2. å¤„ç† genderï¼Œ ä¸‰åˆ†ç±»ï¼Œ m, f o
            gender = value['userinfo']['gender']

            if gender in ['m', 'f']:
                self.gender_frequency.append(gender)
            else:
                value['userinfo']['gender'] = ''

            # 3. å¤„ç† location, åªé™åˆ¶å…·ä½“çœï¼Œä¸æ˜¾ç¤ºå…·ä½“åŸå¸‚ï¼Œ
            location = value['userinfo']['location'].split(' ')[0]
            value['userinfo']['location'] = location
            if location != '':
                self.location_frequency.append(location)

            # 4. å¯¹ description åˆ†è¯
            description = value['userinfo']['description']
            description = jieba.cut(description, cut_all=False)
            description = " ".join(description)
            value['userinfo']['description'] = description

        return comment

    def processing_comments(self, comments):
        # å¤„ç† comemnt
        new_comments = []

        for comment in comments:
            # å¯¹è¯„è®ºæ•°æ®è¿›è¡Œè¿‡æ»¤
            comment = self.processingcomment.filter_comment_content(comment)

            # å¤„ç†é™„åŠ å±æ€§å­—æ®µ
            comment = self.processing_userinfo(comment)

            # è‹¥ comment['content'] ä¸º None, åˆ™åœ¨ comments ä¸­æŠ›å¼ƒæ‰è¯¥è¯„è®º
            content = list(comment.values())[0]['content']
            if content != None:
                new_comments.append(comment)

        # è‹¥ è¿‡æ»¤å comments å†…æ— è¯„è®ºæ•°æ®ï¼Œåˆ™è¿”å›ä¸€ä¸ª []
        return new_comments

    def processing_images(self, images):
        new_images = []
        for item in images:
            image = item['u']
            new_images.append(image)
        return new_images

    def age_default(self):
        # ç»Ÿè®¡å¹´é¾„åˆ†å¸ƒï¼Œè¡¥å……ç¼ºå¤±å€¼
        age_frequency_dict = dict(Counter(self.age_frequency))

        # è·å¾— numpy æ ¼å¼çš„ age å’Œ frequency æ•°æ®
        age = []
        probability = []
        freq_total = sum(age_frequency_dict.values())
        for key, value in age_frequency_dict.items():
            age.append(key)
            probability.append(value/freq_total)
        age = np.array(age)
        probability = np.array(probability)

        check_age = []

        with tqdm(self.Sina.items(), desc='Sina userinfo attribute age default', file=sys.stdout, disable=False) as Sina_dict_iterator:
            for key, value in Sina_dict_iterator:
                comments = value['comments']

                for comment in comments:
                    for comment_key, comment_value in comment.items():
                        if 'age' not in comment_value['userinfo']:
                            # ä½¿ç”¨numpyè¿›è¡ŒæŒ‰æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œè·å¾—ä¸€ä¸ªé»˜è®¤å€¼
                            choice_age = int(list(np.random.choice(a=age, size=1, replace=True, p=probability))[0])
                            comment_value['userinfo'].update({'age': choice_age})
                        check_age.append(comment_value['userinfo']['age'])

        print(Counter(check_age))

    def gender_default(self):
        gender_frequency_dict = dict(Counter(self.gender_frequency))

        # è·å¾— numpy æ ¼å¼çš„ gender å’Œ frequency æ•°æ®
        gender = []
        probability = []
        freq_total = sum(gender_frequency_dict.values())
        for key, value in gender_frequency_dict.items():
            gender.append(key)
            probability.append(value/freq_total)

        check_gender = []
        with tqdm(self.Sina.items(), desc='Sina userinfo attribute gender default', file=sys.stdout, disable=False) as Sina_dict_iterator:
            for key, value in Sina_dict_iterator:
                comments = value['comments']

                for comment in comments:
                    for comment_key, comment_value in comment.items():
                        if comment_value['userinfo']['gender'] not in ['f', 'm']:
                            # ä½¿ç”¨numpyè¿›è¡ŒæŒ‰æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œè·å¾—ä¸€ä¸ªé»˜è®¤å€¼
                            choice_gender = list(np.random.choice(a=gender, size=1, replace=True, p=probability))[0]
                            comment_value['userinfo']['gender'] = choice_gender

                        check_gender.append(comment_value['userinfo']['gender'])

        print(Counter(check_gender))

    def location_default(self):
        location_frequency_dict = dict(Counter(self.location_frequency))

        # è·å¾— numpy æ ¼å¼çš„ gender å’Œ frequency æ•°æ®
        location = []
        probability = []
        freq_total = sum(location_frequency_dict.values())
        for key, value in location_frequency_dict.items():
            location.append(key)
            probability.append(value/freq_total)

        i = 0
        check_location = []

        with tqdm(self.Sina.items(), desc='Sina userinfo attribute location default', file=sys.stdout, disable=False) as Sina_dict_iterator:
            for key, value in Sina_dict_iterator:
                comments = value['comments']

                for comment in comments:

                    i += 1

                    for comment_key, comment_value in comment.items():
                        if comment_value['userinfo']['location'] == '':
                            # ä½¿ç”¨numpyè¿›è¡ŒæŒ‰æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œè·å¾—ä¸€ä¸ªé»˜è®¤å€¼
                            choice_location = list(np.random.choice(a=location, size=1, replace=True, p=probability))[0]
                            comment_value['userinfo']['location'] = choice_location

                        check_location.append(comment_value['userinfo']['location'])

        print(Counter(check_location))
        print('total comment number',i)

    def release_data(self, Sina_filter_attribute_dict):
        """
        æ•°æ®é›†çš„å¤„ç†æ ‡å‡†ï¼šåªä¿ç•™æ–‡ç« å’Œè¯„è®ºå‡ä¸ç©ºçš„æ•°æ®
        ç”±äºå—åˆ°å†…å­˜çš„é™åˆ¶ï¼Œæ‰€ä»¥å°½å¯èƒ½ä¸è¦å¼€è¾Ÿæ–°çš„å­˜å‚¨å•å…ƒï¼Œåº”å°½é‡é‡‡ç”¨åŸæœ‰å˜é‡
        """

        with tqdm(Sina_filter_attribute_dict.items(), desc='Sina userinfo', file=sys.stdout, disable=False) as type_data_dict_iterator:
            for key, value in type_data_dict_iterator:
                title = value['title']
                images = value['images']
                keywords = value['keywords']
                comments = value['comments']

                # å¤„ç†æ–°é—»å†…å®¹ï¼Œè¿‡æ»¤æ‰ä¸è¦çš„å†…å®¹
                # ... å•ç‹¬å¼€è¾Ÿå‡½æ•°å†™è¿‡æ»¤è¡¨è¾¾å¼
                article = value['content']

                # å¤„ç† image ä»…ä¿ç•™url
                images = self.processing_images(images)

                # å¤„ç†æ–‡ç« å†…å®¹
                article = self.processing_article_text(article)

                # å¤„ç† title
                title = self.processing_title_text(title)

                # # å¤„ç† keywords
                keywords = self.processing_keywords(keywords)

                # # å¤„ç†è¯„è®ºå†…å®¹
                comments = self.processing_comments(comments)

                # è·³è¿‡ æ–‡ç« å†…å®¹ä¸ºç©º
                if article == None or article == []:
                    # print('article is None or is []', key)
                    continue

                # è·³è¿‡ è¯„è®ºé›†ä¸ºç©ºçš„
                if comments == None or comments == []:
                    # print('comments is None or is []', key)
                    continue

                # å½“ æ–‡ç«  ä¸ è¯„è®º éƒ½ä¸ä¸ºç©ºæ—¶åˆ™ä¿å­˜æ•°æ®
                if article != None and comments != []:
                    value['images'] = images
                    value['keywords'] = keywords
                    value['comments'] = comments
                    value['content'] = article
                    value['title'] = title

                    self.Sina.update({key: value})
                else:
                    pass

    def forward(self, ):
        Sina_filter_attribute_path = 'Sina/Sina_filter_attribute_712k.json'
        # path name processing
        Sina_path_list = Sina_filter_attribute_path.split('/')
        sub_path = Sina_path_list[0]
        num = Sina_path_list[-1].split('.')[0].split('_')[-1]
        Sina_save_path = os.path.join(PATH, sub_path, f'Sina_{num}.json')

        Sina_filter_attribute_dict = data_read(Sina_filter_attribute_path)

        self.release_data(Sina_filter_attribute_dict)

        # å¯¹ è¡¥å……ç¼ºå¤±å€¼
        self.age_default()
        self.gender_default()
        self.location_default()

        # ä¿å­˜æ•°æ®
        data_save(self.Sina, Sina_save_path)

def main():
    processing = Processing()
    processing.forward()


if __name__ == '__main__':
    main()

